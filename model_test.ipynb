{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import init_detectron\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from random import sample\n",
    "import cv2\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from stacker_net.train import MyDataset, train, train_autoencoder\n",
    "from stacker_net.net import StackerNet\n",
    "from torch import nn\n",
    "import torch\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for root, dirs, files in os.walk('data/data_256'):\n",
    "    if files:\n",
    "        data.append((root, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor, coi = init_detectron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_filename = 'data/selected_images.json'\n",
    "force_overwrite = False\n",
    "\n",
    "if not os.path.isfile(selection_filename) or force_overwrite:\n",
    "    global ratios, data2, paths\n",
    "    ratios = []\n",
    "    for category in tqdm(data):\n",
    "        samples = sample(category[1], 25)\n",
    "        total = len(samples)\n",
    "        positives = 0\n",
    "        for img_name in samples:\n",
    "            img = cv2.imread(os.path.join(category[0], img_name))\n",
    "            outputs = predictor(img)\n",
    "            if coi.intersection(outputs['instances'].pred_classes.cpu().numpy()):\n",
    "                positives += 1\n",
    "        ratios.append(positives / total)\n",
    "    data2 = list(compress(data, np.array(ratios) >= 0.96))\n",
    "    paths = [os.path.join(category[0], file) for category in data2 for file in category[1]]\n",
    "    with open(selection_filename, 'w') as f:\n",
    "        json.dump(paths, f)\n",
    "else:\n",
    "    with open(selection_filename, 'r') as f:\n",
    "        paths = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.PILToTensor(),\n",
    "    torchvision.transforms.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "ds = MyDataset('data/data_256', paths, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_len = int(len(ds) * 0.8)\n",
    "val_len = len(ds) - train_len\n",
    "train_ds, val_ds = random_split(ds, (train_len, val_len))\n",
    "train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, BATCH_SIZE, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369992"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1749c7ef1c47d3bbb436ad6e9e140c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fd87355f61415c80e30e72579a25bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 176.373, val loss = 84.671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460a403536394b41a45c9e314d9c007b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 84.672, val loss = 84.671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9f985a26f34ed0acfc35fbc2d769a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 84.669, val loss = 84.668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82619a761d624738941658fca303a3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/proto/repos/vision-stretcher/model_test.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/proto/repos/vision-stretcher/model_test.ipynb#ch0000010vscode-remote?line=1'>2</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/proto/repos/vision-stretcher/model_test.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/proto/repos/vision-stretcher/model_test.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m train_autoencoder(model, loss_fn, optimizer, train_dl, val_dl, pred_len\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice, print_metrics\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m/mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py:115\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(model, loss_fn, optimizer, train_dl, val_dl, pred_len, epochs, device, print_metrics)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=112'>113</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=113'>114</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=114'>115</a>\u001b[0m \u001b[39mfor\u001b[39;00m X_batch \u001b[39min\u001b[39;00m tqdm(train_dl):\n\u001b[1;32m    <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=115'>116</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=116'>117</a>\u001b[0m     X_batch \u001b[39m=\u001b[39m X_batch\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/notebook.py?line=255'>256</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/notebook.py?line=256'>257</a>\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/notebook.py?line=257'>258</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/notebook.py?line=258'>259</a>\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/notebook.py?line=259'>260</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/notebook.py?line=260'>261</a>\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/std.py?line=1191'>1192</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/std.py?line=1193'>1194</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/std.py?line=1194'>1195</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/std.py?line=1195'>1196</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/std.py?line=1196'>1197</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/tqdm/std.py?line=1197'>1198</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=558'>559</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=559'>560</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=560'>561</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=561'>562</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=562'>563</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataset.py:363\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataset.py?line=360'>361</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataset.py?line=361'>362</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/utils/data/dataset.py?line=362'>363</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m/mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py:19\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=16'>17</a>\u001b[0m item \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpaths[index])\n\u001b[1;32m     <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=18'>19</a>\u001b[0m         item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(item)\n\u001b[1;32m     <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=59'>60</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=60'>61</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=61'>62</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py:122\u001b[0m, in \u001b[0;36mPILToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=109'>110</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=110'>111</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=111'>112</a>\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=112'>113</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=119'>120</a>\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=120'>121</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/transforms.py?line=121'>122</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mpil_to_tensor(pic)\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/functional.py:181\u001b[0m, in \u001b[0;36mpil_to_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=177'>178</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mas_tensor(nppic)\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=179'>180</a>\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=180'>181</a>\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mas_tensor(np\u001b[39m.\u001b[39;49marray(pic, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=181'>182</a>\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(pic\u001b[39m.\u001b[39mgetbands()))\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torchvision/transforms/functional.py?line=182'>183</a>\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "model = StackerNet().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_autoencoder(model, loss_fn, optimizer, train_dl, val_dl, pred_len=32, device=device, print_metrics=True, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080028209c3a45259d1c8db604310e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f8da5307bd4a9ab7df0b05d1728299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 121.554, val loss = 121.711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f440dd7b2a4019994658f776da7eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 121.554, val loss = 121.711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e839a1b2b7324f348f5f41907734568c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 121.554, val loss = 121.711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad6344835c145278aec2d69e03c60fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/proto/repos/vision-stretcher/model_test.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/proto/repos/vision-stretcher/model_test.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/proto/repos/vision-stretcher/model_test.ipynb#ch0000008vscode-remote?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters())\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/proto/repos/vision-stretcher/model_test.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m train(model, loss_fn, optimizer, train_dl, val_dl, pred_len\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice, print_metrics\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m)\n",
      "File \u001b[0;32m/mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py:69\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, optimizer, train_dl, val_dl, pred_len, epochs, device, print_metrics)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=65'>66</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_batch, length\u001b[39m=\u001b[39mpred_len)\n\u001b[1;32m     <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=67'>68</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[0;32m---> <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=68'>69</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=69'>70</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='file:///mnt/c/Users/proto/repos/vision-stretcher/stacker_net/train.py?line=71'>72</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/proto/miniconda3/envs/vision-stretcher/lib/python3.9/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = StackerNet().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train(model, loss_fn, optimizer, train_dl, val_dl, pred_len=32, device=device, print_metrics=True, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "083d3712760fdb628befe5d1b12c121e979c76aa443ff9a7fd57a6967e239b12"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('vision-stretcher')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
